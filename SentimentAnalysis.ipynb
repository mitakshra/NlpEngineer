{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMw6ggk+B4e0o+NB80uFGhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitakshra/NlpEngineer/blob/main/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis**"
      ],
      "metadata": {
        "id": "stmxC92dLmDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF9oqj69Lb9z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Input,Conv1D,GlobalAveragePooling1D,AveragePooling1D\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "Z0qEPELiUaFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punjabi_text = pd.read_excel('/content/drive/MyDrive/NLP/punjabi_data/paa.xlsx')"
      ],
      "metadata": {
        "id": "C7YUXXu2MMNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_punjabi = punjabi_text.iloc[0:5000,0]"
      ],
      "metadata": {
        "id": "nWHHwJADWPGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_punjabi.head()"
      ],
      "metadata": {
        "id": "SBVDygDiYr7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_punjabi.info()"
      ],
      "metadata": {
        "id": "d2vHvTcqWe6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_punjabi.isna().sum()"
      ],
      "metadata": {
        "id": "tyso1K6RWox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df_punjabi.dropna(axis=0)"
      ],
      "metadata": {
        "id": "oqzHB9cMaKhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.isna().sum()"
      ],
      "metadata": {
        "id": "gPafVFJNayb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "id": "m0GsBbUFa3de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(10)"
      ],
      "metadata": {
        "id": "zSkilqo7bBaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = df_punjabi[0]"
      ],
      "metadata": {
        "id": "I-9T1KzlZIQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x1)"
      ],
      "metadata": {
        "id": "FHayfCghZPIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = x1.split()"
      ],
      "metadata": {
        "id": "Mq1krhtSZQw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(l[0])"
      ],
      "metadata": {
        "id": "XHF0COcxZVMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = df_punjabi[1]"
      ],
      "metadata": {
        "id": "doyHTauFdP9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l2 = x2.split()"
      ],
      "metadata": {
        "id": "rYyoOd1TZY16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for indx,xx in enumerate(df_punjabi):\n",
        "  try:\n",
        "    data.append(xx.split())\n",
        "  except:\n",
        "    print(indx)"
      ],
      "metadata": {
        "id": "aIF6p4Y6dVgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(tokens):\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # removing puctuation form each word\n",
        "  tokens = [re_punc.sub('',word) for word in tokens]\n",
        "  tokens = [word.strip() for word in tokens if word not in ['|','']]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "-SF4Mj6DeKfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_lst = [clean_data(tokens[0:-1]) for tokens in data]"
      ],
      "metadata": {
        "id": "XDMSzNyJfpfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_lst_arr = np.asarray(clean_lst)\n",
        "clean_lst_arr"
      ],
      "metadata": {
        "id": "vINGQoIQgcy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_punjabi[0:5]"
      ],
      "metadata": {
        "id": "V8j7kAuuWuQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MARATHI SENTIMENT ANALYSIS**"
      ],
      "metadata": {
        "id": "wAbwBJRipeDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YAbMCjWGphvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.utils import pad_sequences,plot_model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Input,Conv1D,GlobalAveragePooling2D,AveragePooling1D,Activation\n",
        "from tensorflow.keras.layers import Embedding,Flatten,Dropout,Conv2D,AveragePooling2D,Reshape,average,maximum,BatchNormalization\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from numpy import reshape\n"
      ],
      "metadata": {
        "id": "EKQrHEecp-YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_marathi = '/content/drive/MyDrive/NLP/marathi_data/marathi_datum.xlsx'"
      ],
      "metadata": {
        "id": "GoidNqqOpsZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_marathi='/content/drive/MyDrive/NLP/marathi_data/stop_words.xlsx'"
      ],
      "metadata": {
        "id": "CDPHXdnUG0y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_num = '/content/drive/MyDrive/NLP/marathi_data/hindi_numm.xlsx'"
      ],
      "metadata": {
        "id": "X8XAuzujG_p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stpwrds = pd.read_excel(stopwords_marathi)"
      ],
      "metadata": {
        "id": "WKWophaXHI5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hindnum = pd.read_excel(hindi_num)"
      ],
      "metadata": {
        "id": "FNwoDY3-HYFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_marathi = pd.read_excel(path_marathi)"
      ],
      "metadata": {
        "id": "-wMOxrjYqH7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_marathi.head(10)"
      ],
      "metadata": {
        "id": "EscNFtkkqOXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_marathi['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "aHOBpKgA52CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = [text.strip().split() for text in df_marathi['Sentence']]"
      ],
      "metadata": {
        "id": "S2-LEOvhqTsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:5]"
      ],
      "metadata": {
        "id": "eK3IOOZtREnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_creation(data):\n",
        "  vocab_list = [token for tokens in data for token in tokens]\n",
        "  return vocab_list"
      ],
      "metadata": {
        "id": "JrGCa9M8yW3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_num = np.array([0,1,2,3,4,5,6,7,8,9],dtype='str').tolist()"
      ],
      "metadata": {
        "id": "GeZ3Ow0QEWry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_num"
      ],
      "metadata": {
        "id": "GxkSTdAwEnPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hindnum"
      ],
      "metadata": {
        "id": "GE01PexVIucM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_num = df_hindnum.iloc[1:,2].values.tolist()"
      ],
      "metadata": {
        "id": "r7YZnVGYErIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_num = open('/content/drive/MyDrive/NLP/marathi_data/hindi_num.txt')"
      ],
      "metadata": {
        "id": "u8jHUeW8LGfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_num"
      ],
      "metadata": {
        "id": "BUeaOzKNL9KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listt = file_num.read()"
      ],
      "metadata": {
        "id": "faP9qcJoLRUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_hin_num = listt.split('\\n')"
      ],
      "metadata": {
        "id": "tLTAhk69LW4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_hin_num"
      ],
      "metadata": {
        "id": "8xMZ3MwDLeZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = df_stpwrds['Unnamed: 1'].values"
      ],
      "metadata": {
        "id": "ARxfKV5KLp1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punchuations = ['!','@','#','$','%','^','&','(',')','*','|','_','-','+','.',',',';',':',\n",
        "                 '?','/','<','>','','\\'']"
      ],
      "metadata": {
        "id": "GgI3rRFjvp6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_words = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\n",
        "             'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']"
      ],
      "metadata": {
        "id": "rLaAfZ7EWekr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punchuations(tokens):\n",
        "  \n",
        "  final_list = []\n",
        "  for token in tokens:\n",
        "    k = 0\n",
        "    for ch in list(token.strip()):\n",
        "      if ch in punchuations:\n",
        "        k= k+1\n",
        "        break\n",
        "    if k==0:\n",
        "      final_list.append(token)\n",
        "  return final_list"
      ],
      "metadata": {
        "id": "mL39NIBIzsfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punchuations(['Birthday',\n",
        "  'Special',\n",
        "  ':',\n",
        "  'असं',\n",
        "  'करणार',\n",
        "  'कविता',\n",
        "  'मेढेकर',\n",
        "  'खास',\n",
        "  'दिवसाचं',\n",
        "  'सेलिब्रेशन'])"
      ],
      "metadata": {
        "id": "DgUard7n0r7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hindi_num(tokens):\n",
        "  \n",
        "  final_list = []\n",
        "  for token in tokens:\n",
        "    k = 0\n",
        "    for ch in list(token):\n",
        "      if ch in list_hin_num:\n",
        "        k= k+1\n",
        "        break\n",
        "    if k==0:\n",
        "      final_list.append(token)\n",
        "  return final_list\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "rvfYOSSRNqKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_english_num(tokens):\n",
        "  \n",
        "  final_list = []\n",
        "  for token in tokens:\n",
        "    k = 0\n",
        "    for ch in list(token):\n",
        "      if ch in english_num:\n",
        "        k= k+1\n",
        "        break\n",
        "    if k==0:\n",
        "      final_list.append(token)\n",
        "    \n",
        "  return final_list\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "-FrfUce3SG0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_english_words(tokens):\n",
        "  \n",
        "  final_list = []\n",
        "  for token in tokens:\n",
        "    k = 0\n",
        "    for ch in list(token):\n",
        "      if ch in eng_words:\n",
        "        k= k+1\n",
        "        break\n",
        "    if k==0:\n",
        "      final_list.append(token)\n",
        "    \n",
        "  return final_list\n"
      ],
      "metadata": {
        "id": "zOXlA3woWXe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_encoding(dict_mapping_fn,data):\n",
        "  token_id_lst = []\n",
        "  for tokens in data:\n",
        "    token_id = []\n",
        "    for token in tokens:\n",
        "      token_id.append(dict_mapping_fn[token])\n",
        "    token_id_lst.append(token_id)\n",
        "  return token_id_lst"
      ],
      "metadata": {
        "id": "0GsmQr0mLwst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XX = []\n",
        "for tokens in X:\n",
        "  temp = []\n",
        "  for token in tokens:\n",
        "    temp.append(token.strip())\n",
        "  XX.append(temp)"
      ],
      "metadata": {
        "id": "WJSjmaw6Racm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XX[0:5]"
      ],
      "metadata": {
        "id": "LT7JH2w-Rljg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "srt = '\"hello'\n",
        "srt.strip()"
      ],
      "metadata": {
        "id": "60tzRWLKRzLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning_data(tokens):\n",
        "  \n",
        "  tokens = [remove_hindi_num(token) for token in tokens]\n",
        "  tokens = [remove_english_num(token) for token in tokens]\n",
        "  tokens = [remove_punchuations(token) for token in tokens]\n",
        "  tokens = [remove_english_words(token) for token in tokens]\n",
        "  \n",
        "  return tokens\n",
        " \n"
      ],
      "metadata": {
        "id": "8CxXcxMtBvYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data = cleaning_data(X)"
      ],
      "metadata": {
        "id": "CxUhg4g3Obpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data[0:10]"
      ],
      "metadata": {
        "id": "XGYlJOpQOV8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = vocab_creation(clean_data)"
      ],
      "metadata": {
        "id": "Eb-trFXWqhDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_fn = [(tkn,idx) for idx, tkn in enumerate(V)]"
      ],
      "metadata": {
        "id": "AybC5lLUz5pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_mapping_fn = dict(mapping_fn)"
      ],
      "metadata": {
        "id": "izbnIehaNZnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_mapping_fn)"
      ],
      "metadata": {
        "id": "nC6xxzhg0a8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data = create_encoding(dict_mapping_fn,clean_data)"
      ],
      "metadata": {
        "id": "sXq6Qtaty34f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:4]"
      ],
      "metadata": {
        "id": "LBQDRXeUPFUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the max length\n",
        "max_length = max([len(token) for token in encoded_data])"
      ],
      "metadata": {
        "id": "WMlvW3eALukI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq = pad_sequences(encoded_data,maxlen=max_length,padding='post')"
      ],
      "metadata": {
        "id": "bbWSlLZ0Xl07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq[0:10]"
      ],
      "metadata": {
        "id": "8EJUpSYEXxJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_marathi[df_marathi['Sentiment']>3.0]['Sentiment']"
      ],
      "metadata": {
        "id": "wdu5_Xy38kQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(df_marathi['Sentiment'],dtype='int')"
      ],
      "metadata": {
        "id": "uks0vttm3pjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "WlToOsSMVqHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq = Sequential()\n",
        "model_seq.add(Embedding(len(V)+1,512,input_length=max_length))\n",
        "model_seq.add(Flatten())\n",
        "model_seq.add(Dense(128,activation='tanh'))\n",
        "model_seq.add(Dense(3,activation='softmax'))\n",
        "model_seq.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "7VrBmXih7bvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq.fit(padded_seq,y)"
      ],
      "metadata": {
        "id": "Y-p6FE308C-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq.shape"
      ],
      "metadata": {
        "id": "qg5V1qBNKZaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the embedding vector\n",
        "input_shape=(1,max_length,512)\n",
        "input = Input(shape=(1,max_length,512))\n",
        "layer1 = Conv1D(32,3,activation='tanh',input_shape=input_shape[1:])(input)\n",
        "#layer2 = AveragePooling1D(2)(layer1)\n",
        "layer3 = Conv1D(128,3,activation='tanh')(layer1)\n",
        "#layer4 = AveragePooling1D(2)(layer3)\n",
        "model_cnn = Model(inputs=input,outputs=layer3)"
      ],
      "metadata": {
        "id": "JxLyx7D1QZs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn.summary()"
      ],
      "metadata": {
        "id": "Y7uw0AhLeFPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    input = Input(shape=(max_length,))\n",
        "    layer1 = Embedding(len(V)+1,512,input_length=max_length)(input)\n",
        "    layer2 = Reshape((max_length,512,1))(layer1)\n",
        "    layer3 = Conv2D(64,3,activation='tanh',input_shape = (1,max_length,512,1))(layer2)\n",
        "    layer4 = AveragePooling2D(2,2)(layer3)\n",
        "    layer44 = BatchNormalization()(layer4)\n",
        "    layer55 = Activation('tanh')(layer44)\n",
        "\n",
        "    layer5 = Conv1D(128,3,activation='tanh')(layer55)\n",
        "    layer6 = AveragePooling2D(2,2)(layer5)\n",
        "    layer66 = BatchNormalization()(layer6)\n",
        "    layer77 = Activation('tanh')(layer66)\n",
        "    layer7 = Conv1D(256,3,activation='tanh')(layer77)\n",
        "    layer88 = BatchNormalization()(layer7)\n",
        "    layer88 = Activation('tanh')(layer88)\n",
        "    layer8 = Conv1D(512,3,activation='tanh')(layer88)\n",
        "    layer9 = GlobalAveragePooling2D()(layer8)\n",
        "    flat_layer = Flatten()(layer8)\n",
        "    dense_1 = Dense(128,activation='tanh')(flat_layer)\n",
        "    dropout_lyr = Dropout(0.2)(dense_1)\n",
        "    output = Dense(3,activation='softmax')(dense_1)\n",
        "    return Model(input, output)\n",
        "\n",
        "\n",
        "model1 = get_model()\n",
        "model2 = get_model()\n",
        "model3 = get_model()\n",
        "input = Input(shape=(max_length,))\n",
        "y1 = model1(input)\n",
        "y2 = model2(input)\n",
        "y3 = model3(input)\n",
        "outputs = maximum([y1, y2, y3])\n",
        "#outputs = y1\n",
        "\n",
        "ensemble_model = Model(inputs=input, outputs=outputs)"
      ],
      "metadata": {
        "id": "iDd8TY0PmVXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IR3J6aihpLh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(ensemble_model)"
      ],
      "metadata": {
        "id": "wqr0H4BOrHIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_model.fit(padded_seq,y,epochs=50,validation_split=0.3)"
      ],
      "metadata": {
        "id": "7YIeNMmhnba0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyzing Models on SVM**"
      ],
      "metadata": {
        "id": "AuLE2a_XKmaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsOneClassifier,OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score,mean_squared_error"
      ],
      "metadata": {
        "id": "295axf-iK7TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the embedding vector\n",
        "def cnn_context(data):\n",
        "\n",
        "  input = Input(shape=(max_length,))\n",
        "  layer1 = Embedding(len(V)+1,256,input_length=max_length)(input)\n",
        "  layer11 = Reshape((max_length,256,1))(layer1)\n",
        "  layer2 = Conv2D(64,3,activation='tanh',input_shape = (1,max_length,256,1))(layer11)\n",
        "  layer3 = AveragePooling2D(2,2)(layer2)\n",
        "  layer4 = Conv1D(128,3,activation='tanh')(layer3)\n",
        "  layer5 = GlobalAveragePooling2D()(layer4)\n",
        "  layer33 = Flatten()(layer3)\n",
        "\n",
        "  model = Model(inputs=input,outputs=layer5)\n",
        "  model.compile(optimizer='rmsprop',loss='mse')\n",
        "  model_embedding = model.predict(data)\n",
        "\n",
        "  return model_embedding"
      ],
      "metadata": {
        "id": "XewZrIyE9HUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Contex_vectors  = cnn_context(padded_seq)"
      ],
      "metadata": {
        "id": "zsYnAUH_Ktk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to implement we will use powerful FunctionalAPI\n",
        "# as it provide more flexibility and acess\n",
        "inp1 = len(Contex_vectors[0])\n",
        "input_layer = Input(shape = (inp1,))\n",
        "layer1  = Dense(128,activation='tanh')(input_layer)\n",
        "norm11 = BatchNormalization()(layer1)\n",
        "encoder = Dense(64,activation = 'sigmoid')(norm11)\n",
        "\n",
        "# decoder\n",
        "\n",
        "layer3 = Dense(128,activation='relu')(encoder)\n",
        "output = Dense(inp1,activation='linear')(layer3)\n",
        "\n",
        "# building auto encoder model\n",
        "\n",
        "auto_encode = Model(input_layer,output)\n"
      ],
      "metadata": {
        "id": "QNpCC1O4dp0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling the autoencoder\n",
        "auto_encode.compile(optimizer='adam',loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "H_YkiFtEeOoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We implement call back to avoid overfitting\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epoch,logs={}):\n",
        "    if(logs.get('loss')<0.10):\n",
        "      print(\"now loss has been optimized stop training\")\n",
        "      self.model.stop_training  =True"
      ],
      "metadata": {
        "id": "toQFTmwLeOk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the instance of the call back\n",
        "callback = myCallback()"
      ],
      "metadata": {
        "id": "i6veBzNgeVD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_ae = auto_encode.fit(Contex_vectors,Contex_vectors,epochs=500,callbacks=[callback])"
      ],
      "metadata": {
        "id": "9lpr7fUgeZO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_encode = Model(input_layer,encoder)"
      ],
      "metadata": {
        "id": "qlw6aO_vemBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_encode = model_encode.predict(Contex_vectors)"
      ],
      "metadata": {
        "id": "-f2v5iaJeuhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_encode)"
      ],
      "metadata": {
        "id": "tQ4VR0jgf5s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors = x_encode"
      ],
      "metadata": {
        "id": "0VBtJatwgEkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_ohe = LabelBinarizer().fit_transform(y)"
      ],
      "metadata": {
        "id": "7tculBwYM5mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(context_vectors,y_ohe,test_size=0.3)"
      ],
      "metadata": {
        "id": "y7VmcfiFMILF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ovr = OneVsRestClassifier(LinearSVC(random_state=0))"
      ],
      "metadata": {
        "id": "5-ka959uKtiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ovo = OneVsOneClassifier(LinearSVC(random_state=0))"
      ],
      "metadata": {
        "id": "yZDwkL2oKtfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = model_ovr.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "xgIayRyxKtck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ovr = model_ovr.predict(X_test)"
      ],
      "metadata": {
        "id": "uoZBlPeLNeLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('mis classified samples %d' % (y_test != y_pred_ovr).sum())"
      ],
      "metadata": {
        "id": "FRvUiUUXNeFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_test)"
      ],
      "metadata": {
        "id": "qfyNg4ujNeDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of ADA Boost Classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# creating the model of ADA Boost\n",
        "adb_clf = AdaBoostClassifier(n_estimators=200)"
      ],
      "metadata": {
        "id": "9eQ0QGWtNd5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x,test_x,train_y,test_y = train_test_split(Contex_vectors,y,test_size=0.3)"
      ],
      "metadata": {
        "id": "KrmUelSIPD1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the ada boost model with data\n",
        "adb_clf.fit(train_x,train_y)"
      ],
      "metadata": {
        "id": "84dzZYutOrHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_y_adb = adb_clf.predict(test_x)"
      ],
      "metadata": {
        "id": "ho-zyGf-OrEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('mis classified samples %d' % (test_y != predict_y_adb).sum())"
      ],
      "metadata": {
        "id": "vuLjhESrOrBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(test_y,predict_y_adb))"
      ],
      "metadata": {
        "id": "yGRi0PzAOq-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting Classifier**\n"
      ],
      "metadata": {
        "id": "dSdca3SlQbi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier(max_depth=2, n_estimators=120, learning_rate=1.0)"
      ],
      "metadata": {
        "id": "MfRT9mWBOq7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbc.fit(train_x,train_y)"
      ],
      "metadata": {
        "id": "Fd6p7YsUQlFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g= gbc.staged_predict(train_x)\n",
        "print(g)"
      ],
      "metadata": {
        "id": "Loa0JMGYQlC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error = [mean_squared_error(train_y,y_pred_t) for y_pred_t in gbc.staged_predict(train_x)] \n",
        "bst_n_est = np.argmin(error)"
      ],
      "metadata": {
        "id": "2xMuViIWQlAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbct_best = GradientBoostingClassifier(max_depth=2, n_estimators=bst_n_est)\n",
        "gbct_best.fit(train_x,train_y)"
      ],
      "metadata": {
        "id": "rS4YU8nEQk9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y_gbrt = gbct_best.predict(test_x)"
      ],
      "metadata": {
        "id": "cBGrdbflQk6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(test_y,pred_y_gbrt)"
      ],
      "metadata": {
        "id": "OkeneW6rRdE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_lyr = Input(shape=(64,))\n",
        "dense_1 = Dense(128,activation='tanh')(inp_lyr)\n",
        "dropout_lyr = Dropout(0.2)(dense_1)\n",
        "output = Dense(3,activation='softmax')(dense_1)\n",
        "model_op = Model(inp_lyr,output)"
      ],
      "metadata": {
        "id": "_v9GIa5AR06m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_op.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "funSAswVR03e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_op.fit(x_encode,y,validation_split=0.2,epochs=100)"
      ],
      "metadata": {
        "id": "XKxSyd3MR00u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RfZaD9SpR0xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mOWMR8mR0vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lb1I1U_VR0sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VaAJWu-SR0o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85LcKxorR0lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fb6O7QQeR0fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM with Attention Layer**"
      ],
      "metadata": {
        "id": "mWNW25PQCooo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Layer\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy\n",
        "from sklearn.preprocessing import StandardScaler,normalize"
      ],
      "metadata": {
        "id": "LRvNWBYHCs7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(Layer):\n",
        "    \n",
        "  def __init__(self, return_sequences=True):\n",
        "      self.return_sequences = return_sequences\n",
        "      super(Attention,self).__init__()\n",
        "      \n",
        "  def build(self, input_shape):\n",
        "      \n",
        "      self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
        "                              initializer=\"normal\")\n",
        "      self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
        "                              initializer=\"zeros\")\n",
        "      \n",
        "      super(Attention,self).build(input_shape)\n",
        "      \n",
        "  def call(self, x):\n",
        "      \n",
        "      e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "      a = K.softmax(e, axis=1)\n",
        "      output = x*a\n",
        "      \n",
        "      if self.return_sequences:\n",
        "          return output\n",
        "      \n",
        "      return K.sum(output, axis=1)"
      ],
      "metadata": {
        "id": "FeAxY6lkChp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNormLayer(Layer):\n",
        "  def __init__(self,X):\n",
        "    super(AddNormLayer,self).__init__()\n",
        "    self.X = X\n",
        "     \n",
        "  def call(self,v):\n",
        "    return np.array(self.X)+np.array(v)\n",
        "  "
      ],
      "metadata": {
        "id": "6Hkn0WKaLK-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(max_length,))\n",
        "embedded_lyr = Embedding(len(V)+1,256,input_length=max_length)(input)\n",
        "model_embed = Model(inputs=input,outputs=embedded_lyr)\n",
        "model_embed.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
        "embedd_vector = model_embed.predict(padded_seq)\n"
      ],
      "metadata": {
        "id": "m_VgPAdF9HJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_lyr.shape"
      ],
      "metadata": {
        "id": "IxGfvEbGU6ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input1 = Input(shape=(78,256))\n",
        "lstm_1 = Bidirectional(LSTM(256,input_shape=(embedded_lyr.shape),return_sequences=True))(input1)\n",
        "attention = Attention(return_sequences=True)(lstm_1)\n",
        "lstm_2 = LSTM(256,input_shape=(embedded_lyr.shape),return_sequences=True)(attention)\n",
        "model_latent1 = Model(input1,lstm_2)\n",
        "model_latent1.compile(loss='mse', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jRVyhMGPaezJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeded_vec1 = model_latent1.predict(embedd_vector)"
      ],
      "metadata": {
        "id": "YQWvYkr2a4_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add and normalize vectors\n",
        "sc = StandardScaler()\n",
        "add_vecotrs1 = np.array(embeded_vec1)+np.array(embedd_vector)\n",
        "add_vecotrs1.shape"
      ],
      "metadata": {
        "id": "Mp4aFSWhaspH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_vectors = []\n",
        "for sample in add_vecotrs1:\n",
        "  norm_vectors.append(normalize(sample))\n",
        "  "
      ],
      "metadata": {
        "id": "4X05tqdDcysP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(norm_vectors).shape"
      ],
      "metadata": {
        "id": "QWcnDwi-g7Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input2 = Input(shape=(78,256))\n",
        "lstm_3 = Bidirectional(LSTM(256,input_shape=(embedded_lyr.shape),return_sequences=True))(input2)\n",
        "attention = Attention(return_sequences=True)(lstm_3)\n",
        "lstm_4 = LSTM(256,input_shape=(embedded_lyr.shape))(attention)\n",
        "\n",
        "#norm_layer = AddNormLayer(embedd_vector)(lstm_2.numpy())\n",
        "\n",
        "dense_1 = Dense(256,activation='tanh')(lstm_4)\n",
        "output = Dense(3,activation='softmax')(dense_1)\n",
        "model_lstm  = Model(input2,output)\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "EDiWc1InUCuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.fit(np.array(norm_vectors),y,epochs=60)"
      ],
      "metadata": {
        "id": "y0w6o-aMSVQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(embedded_lyr)+padded_seq)"
      ],
      "metadata": {
        "id": "cVF1wmFGTeBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "-Vzwr2mVJUu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "op = model_lstm.predict(padded_seq)"
      ],
      "metadata": {
        "id": "B0Ng2YCKFxAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "op.shape"
      ],
      "metadata": {
        "id": "v-nWOrrGJlry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rW0Cale0J4b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec Algorithm**"
      ],
      "metadata": {
        "id": "r7_djKhM0uOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Cg-GPur-15y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Word2Vec model\n",
        "w2v_model = Word2Vec(clean_data, vector_size=256, window=5, min_count=5, workers=4)"
      ],
      "metadata": {
        "id": "16SoG4gH2BWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a weight matrix for the embedding layer\n",
        "vocab_size = len(V)+1\n",
        "embedding_matrix = np.zeros((vocab_size, 256))"
      ],
      "metadata": {
        "id": "whO-KXwQeD44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_fn = [(tkn,idx) for idx, tkn in enumerate(V)]"
      ],
      "metadata": {
        "id": "G9MDwZMPecT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_mapping_fn = dict(mapping_fn)"
      ],
      "metadata": {
        "id": "ZV4EUR0segl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_mapping_fn"
      ],
      "metadata": {
        "id": "3qSlUUsIfQIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a weight matrix for the embedding layer\n",
        "for word, i in dict_mapping_fn.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]"
      ],
      "metadata": {
        "id": "pX4XlOBqfISB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 256, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(Conv1D(128, 5, activation='tanh'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Conv1D(256, 5, activation='tanh'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "metadata": {
        "id": "kBV9H0FyfeKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(padded_seq,y, epochs=500, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "VnTDPzR5fnqH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}