{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPTW4MEfTzyHd8T8tkoNiAP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitakshra/NlpEngineer/blob/Machine-Translation/NLP_ITV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft3hUxz4IjlH"
      },
      "outputs": [],
      "source": [
        "strr = \"HI how are you weather is looking cool\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "l = [[1,2,3],[2,3,4]]\n",
        "print(np.mean(l))"
      ],
      "metadata": {
        "id": "V05fA3AG30m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = strr.split()"
      ],
      "metadata": {
        "id": "Z5huAVeWI5Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "rkBY8BmzI8Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.random.randn(10,20)"
      ],
      "metadata": {
        "id": "tbtEp0FhKkjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization using tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "examples = ['We are learning natural language processing','we want to become expert of text processing',\n",
        "            'Baba baba black sheep','welcome to word of data']\n",
        "\n",
        "# build a tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(examples)\n"
      ],
      "metadata": {
        "id": "Rg5GLGMLQ6o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dictionary of the examples has been created\n",
        "word_idxs = tokenizer.word_index\n",
        "print(word_idxs)"
      ],
      "metadata": {
        "id": "lZK01mGtUTjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_indexes = [[1,6,7,8,9,2],[]]"
      ],
      "metadata": {
        "id": "XGHuv3kTV0PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectors of the text\n",
        "encoded_seq = tokenizer.texts_to_sequences(examples)"
      ],
      "metadata": {
        "id": "yyO4YABHXAQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_seq"
      ],
      "metadata": {
        "id": "W1vw5KKnXgHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq1 = pad_sequences(encoded_seq,padding='pre')"
      ],
      "metadata": {
        "id": "CatxIaKrYuOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq1"
      ],
      "metadata": {
        "id": "73AGe1WIY8ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq2 = pad_sequences(encoded_seq,padding='post')"
      ],
      "metadata": {
        "id": "G8NP5BnDY4i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq2"
      ],
      "metadata": {
        "id": "kycyWx0HY_UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = ['Machine Learning expert are good']\n",
        "\n",
        "enocoded_new = tokenizer.texts_to_sequences(sent)"
      ],
      "metadata": {
        "id": "cs-zPGxlZLHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enocoded_new"
      ],
      "metadata": {
        "id": "WA53Fy4uaum8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update my tokenizer for handling OOV token\n",
        "tokenizer1 = Tokenizer(oov_token='<OOV>')"
      ],
      "metadata": {
        "id": "5L6SmDuobDsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocabulary again for the whole corpus\n",
        "tokenizer1.fit_on_texts(examples)"
      ],
      "metadata": {
        "id": "IPSj5vgmbVuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new dictionary will come with OOV token\n",
        "word_idx_new = tokenizer1.word_index\n",
        "print(word_idx_new)"
      ],
      "metadata": {
        "id": "zIyv2fW9bpKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enocoded_new1 = tokenizer1.texts_to_sequences(sent)"
      ],
      "metadata": {
        "id": "8wvvNBzMcErm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enocoded_new1"
      ],
      "metadata": {
        "id": "yYSD_bENcNtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Tool kit"
      ],
      "metadata": {
        "id": "fHKkxVSjeOj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "NFi-P7I_cf9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "un7woszCcf6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a sentence\n",
        "sentence = ' I am Rituraj Dixit an NLP Engineer woking on Sentiment Analysis and Translation models'\n",
        "tokens = nltk.word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "Uf6hLBMhcf2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "zvROLNGncfy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part of speech tagging\n",
        "from nltk.tag import pos_tag\n",
        "pos_tagg = pos_tag(tokens,tagset='universal')"
      ],
      "metadata": {
        "id": "eW_tCu1le6Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tagg"
      ],
      "metadata": {
        "id": "AOrGQXqmfQ5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_P6L2Fk9gE6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mannual Tokenization**"
      ],
      "metadata": {
        "id": "Y_RtwhQhpSe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_txt = '/content/drive/MyDrive/NLP/metaphorph.txt'"
      ],
      "metadata": {
        "id": "1Pt4KZE0m2me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(path_txt,'rt')\n",
        "txt = file.read()\n",
        "file.close()"
      ],
      "metadata": {
        "id": "2MTNHlYJpidQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(txt)"
      ],
      "metadata": {
        "id": "CS0Byw0spsrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the words on the basis of whitespaces\n",
        "words = txt.split()\n",
        "words[0:20]"
      ],
      "metadata": {
        "id": "QmNtLelRp4My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# better approach is to use the regular expression which will filter only those \n",
        "# owrds which will fit into the pattern\n",
        "import re\n",
        "words_reg = re.split(r'\\W+',txt)\n",
        "words_reg[0:20]\n"
      ],
      "metadata": {
        "id": "k8FjFX0CrwqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_reg)"
      ],
      "metadata": {
        "id": "Lqa7408etShi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the text\n",
        "words_norm = [word.lower() for word in words_reg]"
      ],
      "metadata": {
        "id": "zRNDqTDhsxqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_norm[0:20]"
      ],
      "metadata": {
        "id": "9JGZKJg7tIxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_norm)"
      ],
      "metadata": {
        "id": "eBZaFJQ-tOEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization using nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens_nltk = word_tokenize(txt)"
      ],
      "metadata": {
        "id": "3YSUGjDmtv6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_nltk[0:100]"
      ],
      "metadata": {
        "id": "QbKYlFdQuqLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens_nltk)"
      ],
      "metadata": {
        "id": "fjAwUEwivavt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove all tokens that are not alphabetic\n",
        "words_nltk = [word for word in tokens_nltk if word.isalpha()]"
      ],
      "metadata": {
        "id": "ZKti6uQWu6vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_nltk)"
      ],
      "metadata": {
        "id": "QO4RX0IlvR4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering out the stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "3LMSP2XqwKYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "id": "XQDOFoJayWPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stop_words))"
      ],
      "metadata": {
        "id": "wz2mCCNwzW95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refined_tokens  = [word for word in words_nltk if word not in stop_words]"
      ],
      "metadata": {
        "id": "y09gc_4w1g7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "# stemming of words\n",
        "porter = PorterStemmer()\n",
        "stemmed_words  = [porter.stem(word) for word in refined_tokens]"
      ],
      "metadata": {
        "id": "dFUShC56zdB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words[0:20]"
      ],
      "metadata": {
        "id": "UM02JESL1-Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to Prepare the Text Data using Scikit-learn**"
      ],
      "metadata": {
        "id": "hsQ5YXeR22z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# list of text documents\n",
        "text_doc = [\"The quick brown fox jumped over the lazy dog\"]\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text_doc)\n",
        "# summarize the result\n",
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "id": "om-j-VTp3EMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform a single document\n",
        "vector = vectorizer.transform(text_doc)"
      ],
      "metadata": {
        "id": "yHTScgg29H77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector.toarray()"
      ],
      "metadata": {
        "id": "7Xw9Ud6q9YQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"when Gregor Samsa woke from troubled dreams\",\n",
        "          \"he found himself transformed in his bed into a horrible vermin\",\n",
        "          \"The bedding was hardly able to cover it and seemed ready to slide off any moment.\",\n",
        "          \"His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.\"]"
      ],
      "metadata": {
        "id": "-VB_xntE9iaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer1 = CountVectorizer()\n",
        "vectors = [vectorizer1.fit_transform(c) for c in corpus]"
      ],
      "metadata": {
        "id": "bxAW6Rih-HpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for v in vectors:\n",
        "  print(v.toarray())"
      ],
      "metadata": {
        "id": "lffuhwbB-wgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how to use tfids\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vec  =TfidfVectorizer()\n",
        "# vocabulary first\n",
        "vocab_tfidf = tfidf_vec.fit(corpus)"
      ],
      "metadata": {
        "id": "Nr4H9BvF_P1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the vocabulary\n",
        "print(vocab_tfidf.vocabulary_)"
      ],
      "metadata": {
        "id": "IKihud1Q_Pum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector1  = tfidf_vec.transform([corpus[0]])"
      ],
      "metadata": {
        "id": "7lRoYHQ0AbgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector1.toarray())"
      ],
      "metadata": {
        "id": "RBhBVK-PAwZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bag-of-Words Model"
      ],
      "metadata": {
        "id": "kiFeS3wNBqWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o-xiN12NBuLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_imdb = '/content/drive/MyDrive/NLP/imdb_dataset/Train.csv'"
      ],
      "metadata": {
        "id": "xBrFylTTVdPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D,Dense,AveragePooling1D,Flatten,Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "6SkaiElnWfiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_imdb = pd.read_csv(path_imdb)"
      ],
      "metadata": {
        "id": "j1DAg3BSXfM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_imdb.head()"
      ],
      "metadata": {
        "id": "N_ulzQ0uXo1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_imdb"
      ],
      "metadata": {
        "id": "osApQiFpqc1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(path):\n",
        "  file_1 = open(path,'rt')\n",
        "  text = file_1.read()\n",
        "  file_1.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "tApRB8HGd8an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0: negative comments\n",
        "# 1: Postive comments\n",
        "# clean the data\n",
        "def clean_doc(doc):\n",
        "  # split the tokens using white spaces\n",
        "  tokens = re.split(r'\\W+',doc)\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # removing puctuation form each word\n",
        "  tokens = [re_punc.sub('',word) for word in tokens]\n",
        "  #remove tokens which are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "  # filter short words\n",
        "  tokens = [token for token in tokens if len(token)>2]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "VaXUdAmmXwBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_doc_to_vocab(doc,vocab):\n",
        "  # load to clean the doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # clean tokens are get updated to the vocabularly \n",
        "  vocab.update(tokens)\n",
        "  return vocab\n"
      ],
      "metadata": {
        "id": "G8eXWSapa7DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_tokens(tokens,filename):\n",
        "  data = '\\n'.join(tokens)\n",
        "  # open a file\n",
        "  vocab_file = open(filename,'w')\n",
        "  # write the tokens in the file\n",
        "  vocab_file.write(data)\n",
        "  # cloase file\n",
        "  vocab_file.close()"
      ],
      "metadata": {
        "id": "hbj7pgAKi41K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metamorphsis 2\n",
        "path_sample = '/content/drive/MyDrive/NLP/Metaphorphis2.txt'"
      ],
      "metadata": {
        "id": "-iUy0S9MdlEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_txt = load_doc(path_sample)"
      ],
      "metadata": {
        "id": "-o0JHkf8d2-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_meta = clean_doc(sample_txt)"
      ],
      "metadata": {
        "id": "-WNDhk-kdyBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_meta"
      ],
      "metadata": {
        "id": "htKIgrwgewTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab1 = Counter()"
      ],
      "metadata": {
        "id": "PL44GAz4kjwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = df_imdb.iloc[1:4,0]"
      ],
      "metadata": {
        "id": "ajfav9cvYmKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr"
      ],
      "metadata": {
        "id": "K-NhNDegYu-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab1 = Counter()"
      ],
      "metadata": {
        "id": "Se6hlqfnb_Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in arr.values:\n",
        "  add_doc_to_vocab(sample_txt,vocab1)"
      ],
      "metadata": {
        "id": "HC18FaDPgzmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab1"
      ],
      "metadata": {
        "id": "lucwIxbzY8-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minimum occurence=3\n",
        "min_occurence = 3\n",
        "filter_vocab_wrds = [w for w,c in vocab1.items() if c>=min_occurence]"
      ],
      "metadata": {
        "id": "MqO_TeGAcQ5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the filtered words in vocabulary in seprate text file\n",
        "path_vocab = '/content/drive/MyDrive/NLP/Meta_vocab.txt'\n",
        "save_tokens(filter_vocab_wrds,path_vocab)"
      ],
      "metadata": {
        "id": "2I81fPHRi2WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Text prepration using Keras API**"
      ],
      "metadata": {
        "id": "Ho-vCvjg0Oq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import plot_model\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "TS-ISN-f0ekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"Good work keep it up\",\n",
        "    \"Very very good effort\",\n",
        "    \"Nice work\",\n",
        "    \"Excellent job\",\n",
        "    \"enjoying NLP\",\n",
        "    \"Learning new tranformers\"\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "wBgHQSHa5HLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an istance of Tokenizer\n",
        "tk = Tokenizer()\n",
        "# create the vocabulary\n",
        "tk.fit_on_texts(docs)\n",
        "# we use word_index to see the vocabulary\n",
        "vocab = tk.word_index\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "XXoDAQCh55Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we have to convert these text elts into sequences\n",
        "seq  = tk.texts_to_sequences(docs)"
      ],
      "metadata": {
        "id": "6C9tzucM62k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "rUc1bbY47gJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq"
      ],
      "metadata": {
        "id": "GXjyhoSP7QIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq1 = tk.texts_to_matrix(docs,mode='binary')"
      ],
      "metadata": {
        "id": "iJFZUZ477SiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq1"
      ],
      "metadata": {
        "id": "n7X0Estv7XN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2 = tk.texts_to_matrix(docs,mode='count')"
      ],
      "metadata": {
        "id": "8uUu0tRw8Y0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2"
      ],
      "metadata": {
        "id": "Hj491reP8gjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq3 = tk.texts_to_matrix(docs,mode='tfidf')"
      ],
      "metadata": {
        "id": "XZ7uOrfj8p8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq3"
      ],
      "metadata": {
        "id": "VUfo1p0a8xia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Sentiment Analysis Model using Keras API"
      ],
      "metadata": {
        "id": "P3Btf4lDmaRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yl3cuE0x9Sfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nuG3IVPB98wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_imdb = '/content/drive/MyDrive/NLP/imdb_dataset/Train.csv'"
      ],
      "metadata": {
        "id": "vsxPd00q9yce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_movie = pd.read_csv(path_imdb)"
      ],
      "metadata": {
        "id": "y812buh291Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_movie['text']"
      ],
      "metadata": {
        "id": "510O_zNU-A9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary\n",
        "tk_movie = Tokenizer()\n",
        "tk_movie.fit_on_texts(X)"
      ],
      "metadata": {
        "id": "eNQAHhSM-H7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_movie = tk_movie.word_index"
      ],
      "metadata": {
        "id": "3wwZQXEv-X2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_movie)"
      ],
      "metadata": {
        "id": "iCl8x-e7-jQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = len(vocab_movie)\n",
        "print(max_len)"
      ],
      "metadata": {
        "id": "fACx-6bh-tDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will create the matrices for all the three modes and then we will analyze the accuracy\n",
        "seq_count = tk_movie.texts_to_matrix(X,mode='count')"
      ],
      "metadata": {
        "id": "fMWklESO-2yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_tfidf= tk_movie.texts_to_matrix(X,mode=\"tfidf\")"
      ],
      "metadata": {
        "id": "oTGwDaA4_Xjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_bin = tk_movie.texts_to_matrix(X,mode='binary')"
      ],
      "metadata": {
        "id": "5ejsMEFaBh-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df_movie['label']"
      ],
      "metadata": {
        "id": "ANZy3YZiB17N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x,text_x,train_y,test_y = train_test_split(seq_count,Y,test_size=0.3)"
      ],
      "metadata": {
        "id": "GYLtuXF6C1Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the model\n",
        "model_movie = Sequential()\n",
        "model_movie.add(Dense(32,input_shape=(max_len+1,),activation='tanh'))\n",
        "model_movie.add(Dense(1,activation='sigmoid'))\n",
        "model_movie.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "model_movie.summary()"
      ],
      "metadata": {
        "id": "38HpfEoCDh9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_movie.fit(train_x,train_y,epochs=10)"
      ],
      "metadata": {
        "id": "wEDyyK5pFcyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_tf,text_x_tf,train_y_tf,test_y_tf = train_test_split(seq_tfidf,Y,test_size=0.3)"
      ],
      "metadata": {
        "id": "uukspD79JRUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_movie.fit(train_x,train_y,epochs=10)"
      ],
      "metadata": {
        "id": "yWBk8XjXJRpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Word Embedding Models"
      ],
      "metadata": {
        "id": "kgVkyxw6O1KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA \n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "NF7MMilMNhwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a training data\n",
        "sentences = [['this','is','a','sentence','of','word2vec'],\n",
        "             ['this', 'is','a','second','sentence', 'of','word2vec'],\n",
        "             ['now', 'we','are','learning','word','embedding', 'for','NLP'],\n",
        "             ['and','this','is','the','final','sentence'],\n",
        "             ['the','word','embedding','is','used','to','grasp','semantic','information']]"
      ],
      "metadata": {
        "id": "ziU8VIyWO8kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model \n",
        "model_w2v = Word2Vec(sentences,min_count=1)"
      ],
      "metadata": {
        "id": "_lGS_so9PHHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the model\n",
        "print(model_w2v)"
      ],
      "metadata": {
        "id": "yc1iYp9wPKcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the vocabulary\n",
        "words = list(model_w2v.wv.vocab)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "72Dk81QBPSTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# acess a word vector\n",
        "print(len(model_w2v['to']))"
      ],
      "metadata": {
        "id": "l3V3XbkFPXjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v['to']"
      ],
      "metadata": {
        "id": "2DokYS_1PZ-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visulaizing the model\n",
        "X = model_w2v[model_w2v.wv.vocab]"
      ],
      "metadata": {
        "id": "cBybMcTzPovF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "zHwLNQ4EPxhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "RvxXnBDdP1tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = X[:,0:2]"
      ],
      "metadata": {
        "id": "0-HO_1bYP8O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1"
      ],
      "metadata": {
        "id": "Cx9MU_QWQAUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# principal component analysis is the effective technique to reduce the dimensions of the data \n",
        "# without loosing the meaning\n",
        "pca = PCA(n_components=2)\n",
        "reduced_mtx = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "zqO11CnEQKJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_mtx"
      ],
      "metadata": {
        "id": "7km_75sXTVp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_mtx.shape"
      ],
      "metadata": {
        "id": "B0HZn1v8Tak3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyplot.scatter(reduced_mtx[:,0],reduced_mtx[:,1])"
      ],
      "metadata": {
        "id": "v-QrqVK7Tee9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words  = list(model_w2v.wv.vocab)"
      ],
      "metadata": {
        "id": "IElulVyRTgQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,word in enumerate(words):\n",
        "  pyplot.annotate(word,xy=(reduced_mtx[i,0],reduced_mtx[i,1]))\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "1DCI2mOETpGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis using financial sentiment analysis"
      ],
      "metadata": {
        "id": "gV3LEiA5z7os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fOyun2jN0Fkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA \n",
        "from matplotlib import pyplot\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Conv1D,Dense,AveragePooling1D,Flatten,Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import plot_model,pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "RLHZIRKT0QEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "3NiWERff3bwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_financial = '/content/drive/MyDrive/NLP/financial_sentiment.csv'"
      ],
      "metadata": {
        "id": "zLXIq_HJ1HbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_financial = pd.read_csv(path_financial)"
      ],
      "metadata": {
        "id": "avYVbGq71QUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_financial.shape"
      ],
      "metadata": {
        "id": "G_mbzRFl1g8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df_financial.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "JE2DLPJZ1nqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "0cOjVF8o17Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "TwPYart417N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_doc(doc):\n",
        "  # split the tokens using white spaces\n",
        "  tokens = re.split(r'\\W+',doc)\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # removing puctuation form each word\n",
        "  tokens = [re_punc.sub('',word) for word in tokens]\n",
        "  #remove tokens which are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "  # filter short words\n",
        "  tokens = [token for token in tokens if len(token)>=2]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "ezrS22wl2u41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1['Sentence']"
      ],
      "metadata": {
        "id": "GZ6HaFDG2-Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[4106]"
      ],
      "metadata": {
        "id": "zf29KyUfl7Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens = [clean_doc(sent) for sent in X ]"
      ],
      "metadata": {
        "id": "VeEPvaUP1y3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens"
      ],
      "metadata": {
        "id": "IXDVRMe539gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the sentiment model \n",
        "financial_w2v = Word2Vec(sentence_tokens,min_count=1)"
      ],
      "metadata": {
        "id": "klOvgV6F4MI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the vocabulary\n",
        "financial_words = list(financial_w2v.wv.vocab)\n",
        "print(financial_words)"
      ],
      "metadata": {
        "id": "KF7E4DcZ4Bf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "id": "idhdpYKdKgYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = []\n",
        "index=1\n",
        "for tkn_lst in sentence_tokens:\n",
        "  lst = []\n",
        "  for word in tkn_lst:\n",
        "    lst.append(financial_w2v[word])\n",
        "  if len(lst)==0:\n",
        "    print(index)\n",
        "  X_data.append(lst)\n",
        "  index=index+1\n"
      ],
      "metadata": {
        "id": "DYa7zHDAKETF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens[4107]"
      ],
      "metadata": {
        "id": "aSZOUktqlpCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens[4106]"
      ],
      "metadata": {
        "id": "SjZXmLNxls8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data_flatten = []\n",
        "for vectors in X_data:\n",
        "  if len(vectors)>=1:\n",
        "    X_data_flatten.append(list(np.concatenate(vectors).flat))"
      ],
      "metadata": {
        "id": "nrpMLg-0j1vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_data_flatten)"
      ],
      "metadata": {
        "id": "H_CSAWnOkwRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df1['Sentiment'].map({'neutral':0,'positive':1,'negative':2})"
      ],
      "metadata": {
        "id": "tNm5iZVXLMLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(Y)"
      ],
      "metadata": {
        "id": "5m7BcPPWOVnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = Y.drop(Y.index[4106])"
      ],
      "metadata": {
        "id": "tHfv51tJnGsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y)"
      ],
      "metadata": {
        "id": "nCNutB7fnYrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(vector) for vector in X_data_flatten])"
      ],
      "metadata": {
        "id": "HylvdOHLngXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length"
      ],
      "metadata": {
        "id": "_rX0t0nHngUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_length = min([len(vector) for vector in X_data_flatten])"
      ],
      "metadata": {
        "id": "YCs_wPL9ngIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_length"
      ],
      "metadata": {
        "id": "Dd3gxDVLn527"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_vector = []"
      ],
      "metadata": {
        "id": "oMDYCrFoozh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for vector in X_data_flatten:\n",
        "  diff = max_length-len(vector)\n",
        "  for val in range(diff):\n",
        "    vector.append(0)\n",
        "  final_vector.append(vector)\n"
      ],
      "metadata": {
        "id": "z_QAgZWZn8aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_length = min([len(vector) for vector in final_vector])"
      ],
      "metadata": {
        "id": "CBGoxxAeo31f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_length"
      ],
      "metadata": {
        "id": "5UYJmoiao9z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we build the autoencoder for dimensionality reduction\n"
      ],
      "metadata": {
        "id": "dW8onZSGpAiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implemeting Financial Setiment using tensorflow"
      ],
      "metadata": {
        "id": "6u2nk0Kb58Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tk_fin = Tokenizer()"
      ],
      "metadata": {
        "id": "3FFFp1iY55qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk_fin.fit_on_texts(df1['Sentence'])"
      ],
      "metadata": {
        "id": "MITFNmJs55eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary of financial setiments\n",
        "vocab_financial = tk_fin.word_index"
      ],
      "metadata": {
        "id": "3amqEuJq6QvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can create sequences\n",
        "X_seq = tk_fin.texts_to_sequences(df1['Sentence'])"
      ],
      "metadata": {
        "id": "6ov3Xs5T6d5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding\n",
        "# max size\n",
        "max_len = max([len(x) for x in X_seq])"
      ],
      "metadata": {
        "id": "AxY3N_lK6s7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_padded = pad_sequences(X_seq,padding='post')"
      ],
      "metadata": {
        "id": "CGk945Qm7Ln8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model\n",
        "model_fin = Sequential()\n",
        "model_fin.add(Embedding(len(vocab_financial)+1,100,input_length=max_len))\n",
        "model_fin.add(Conv1D(32,5,activation='tanh'))\n",
        "model_fin.add(AveragePooling1D(2))\n",
        "model_fin.add(Flatten())\n",
        "model_fin.add(Dense(32,activation='tanh'))\n",
        "model_fin.add(Dense(3,activation='softmax'))\n",
        "model_fin.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "R4hMvkCC7fU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fin.summary()"
      ],
      "metadata": {
        "id": "SrrYy54c9S5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model_fin)"
      ],
      "metadata": {
        "id": "miRuqP519XTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df1['Sentiment'].map({'neutral':0,'positive':1,'negative':2})"
      ],
      "metadata": {
        "id": "-BQ5lCqm9cQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "tvkoD16L9x8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data\n",
        "train_x,test_x,train_y,test_y = train_test_split(X_padded,Y,test_size=0.3)"
      ],
      "metadata": {
        "id": "nmappz2p99Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "history_fin = model_fin.fit(train_x,train_y,epochs=10)"
      ],
      "metadata": {
        "id": "Ux5p5NWb9_Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sequence Models"
      ],
      "metadata": {
        "id": "AX43yNeuA8rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recurrent Neural Networks**"
      ],
      "metadata": {
        "id": "6mJyFPHKBC4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "RGF3RZn6BKSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_0 = np.random.randn(4)"
      ],
      "metadata": {
        "id": "NM6VQ0ecBAn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_0"
      ],
      "metadata": {
        "id": "oitb027kBUuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_aa = np.random.randn(4,4)"
      ],
      "metadata": {
        "id": "Q08L__SiCsgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_aa"
      ],
      "metadata": {
        "id": "mvvRhjKmC10S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_1 = np.random.randn(6)"
      ],
      "metadata": {
        "id": "_ViXoWsDEKMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_xa = np.random.randn(6,4)"
      ],
      "metadata": {
        "id": "nHZl9BAyEPn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_a = np.random.rand(4)"
      ],
      "metadata": {
        "id": "o-9dLwQtE24W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_1 = np.tanh(np.dot(a_0,W_aa)+ np.dot(x_1,W_xa) + b_a)"
      ],
      "metadata": {
        "id": "CDTpc40GEYaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_1"
      ],
      "metadata": {
        "id": "4PK40qu3E-On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight martix for output and let us suppose we have only three classes\n",
        "# PRN, NN, VB\n",
        "W_ay = np.random.rand(4,3)\n",
        "b_y = np.random.rand(3)"
      ],
      "metadata": {
        "id": "oHKHxDnMFljg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)"
      ],
      "metadata": {
        "id": "dvAt-LegGFZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val = np.dot(a_1,W_ay)+b_y"
      ],
      "metadata": {
        "id": "YsWQHmUUGpXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val"
      ],
      "metadata": {
        "id": "BgeEUgm-G2Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = softmax(y_val)"
      ],
      "metadata": {
        "id": "_YlzfJAGG5mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "eEKQPWOfHA6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN for Financial Sentiment Analysis"
      ],
      "metadata": {
        "id": "ODB_Ov4X53FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the normalize data is there in X_padded\n",
        "# create a RNN model\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "embdedding_size = 32\n",
        "vocab_size = len(vocab_financial)+1\n",
        "# max size\n",
        "max_len = max([len(x) for x in X_seq])\n",
        "\n"
      ],
      "metadata": {
        "id": "pOKV38qg0K_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(vocab_size,embdedding_size,input_length= max_len ))\n",
        "model_rnn.add(SimpleRNN(64))\n",
        "model_rnn.add(Dense(32,activation='tanh'))\n",
        "model_rnn.add(Dense(1,activation='sigmoid'))\n",
        "model_rnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RKZXWpzJ72fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn.summary()"
      ],
      "metadata": {
        "id": "TtEG9Xbj9_PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x,test_x,train_y,test_y = train_test_split(X_padded,Y,test_size=0.3)"
      ],
      "metadata": {
        "id": "eIKxP5YH-FKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the mdoel\n",
        "history = model_rnn.fit(train_x,train_y,epochs=10)"
      ],
      "metadata": {
        "id": "mKiSXFmN-efb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU Cells (Gated Recurrent Units)**"
      ],
      "metadata": {
        "id": "GDIIt1E_SLvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "UJeB3Tim-X2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden state information\n",
        "a_t0 = np.random.randn(5)\n",
        "# weight matrices for relevance gate\n",
        "W_aa_r = np.random.randn(5,5)\n",
        "W_xa_r = np.random.randn(5,5)\n",
        "#weight matrices for update gates\n",
        "W_aa_u = np.random.randn(5,5)\n",
        "W_xa_u = np.random.randn(5,5)\n",
        "#weight matrices for c_tilde gates\n",
        "W_aa_c = np.random.randn(5,5)\n",
        "W_xa_c = np.random.randn(5,5)\n",
        "# input information \n",
        "x_t = np.random.rand(5)\n",
        "# bias initialization\n",
        "b_r = np.random.rand(5)\n",
        "b_u = np.random.rand(5)\n",
        "b_c = np.random.rand(5)\n",
        "\n"
      ],
      "metadata": {
        "id": "nDI1wedqScY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))"
      ],
      "metadata": {
        "id": "7BxMAEUtUYhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# relevance gate value\n",
        "lamda_r = np.dot(a_t0,W_aa_r)+np.dot(x_t,W_xa_r)+b_r"
      ],
      "metadata": {
        "id": "pwTn7M2fTk4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lamda_r_sig = [sigmoid(val) for val in lamda_r]"
      ],
      "metadata": {
        "id": "O9TDXRfYUw23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lamda_r_sig"
      ],
      "metadata": {
        "id": "ayuhQRQ0VDQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update gate value\n",
        "lamda_u = np.dot(a_t0,W_aa_u)+np.dot(x_t,W_xa_u)+b_u"
      ],
      "metadata": {
        "id": "28BhGgWGVMS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lamda_u_sig = [sigmoid(val) for val in lamda_u]"
      ],
      "metadata": {
        "id": "kEYf3PDnVaWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lamda_u_sig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjgWW1WlVgUQ",
        "outputId": "5fd61213-f114-4b30-9aa8-8dcb5ef23756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9624778335224151,\n",
              " 0.9936441097907869,\n",
              " 0.9960408284440706,\n",
              " 0.9721185397999369,\n",
              " 0.9075891678526896]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "6_V7mS5GWFAT",
        "outputId": "cd7b47e3-740e-483c-96ab-8f8658250d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b7a4a6bcb702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c_tilde\n",
        "elt_mul = np.array([lamda_u_sig[i]*a_t0[i] for i in range(len(a_t0))])\n",
        "c_tilde = np.tanh(np.dot(W_aa_c,elt_mul)+np.dot(x_t,W_xa_c)+b_c)"
      ],
      "metadata": {
        "id": "OWg-nXiwVjDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_t = "
      ],
      "metadata": {
        "id": "yLWcaHq2W27r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}